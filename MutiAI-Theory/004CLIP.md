# CLIP
## CLIP
- 이미지와 텍스트를 연결하는 모델
- 4억 개 이상의 이미지-텍스트 쌍으로 학습된 멀티모달 대표 모델
- 이미지와 텍스트를 같은 벡터 ㄱ오간에 매핑하여 서로 비교, 검새 가능하게 만듦

**핵심 아이디어**
- 이미지 인코더 + 텍스트 인코더
- 대조 학습으로 같은 쌍은 가깝게, 다른 쌍은 멀게

### 특징
**1. 이미지 텍스트 검색 시스템**
- 텍스트로 이미지 검색 혹은 이미지로 유사 이미지 검색

**2. 제로 샷 이미지 분류**
- 새로운 카테고리라 하더라도, 레이블 이름을 문장으로 만들어 분류 가능

**3. 콘텐츠 필터링 및 태깅 자동화**
- 이미지와 설명 테스트의 일치 여부 확인
- 자동으로 이미지에 태그 생성

### 구성 요소
- 이미지 인코더
vit 또는 resNet 기반
이미지를 고정 길이 벡터로 변환

- 텍스트 인코더
트랜스폼 기반
문장을 고정 길이 벡터로 변환

- 공통 임베딩 공간
이미지 텍스트 벡터를 동일한 차원의 벡터 공간으로 투영

---
## BLIP
- 웹에서 모은 노이즈가 많은 캡션 데이터 문제
웹 데이터에는 고양이 사진인데 캡션이 귀여운 동물처럼 모호하거나 멋진 풍경처럼 아무  상관 없는 문장도 섞여있음 -> 이미지와 텍스트의 관계를 제대로 학습하지 못함

- CaoFilt : BLIP는 이미 존재하는 사전학습 모델을 이요해 캡션 품질을 평가하고 걸러내는 과정
- captioner 먼저 모델이 미지를 보고 캡션 생성
- 판별기 
    - 기존 웹 캡션과 생성된 캡션을 비교하여 이 캡션이 이미지 내용과 얼마나 잘 맞느가를 점수로 평가 점수가 낮은 캡션은 학습 데이터에서 제외