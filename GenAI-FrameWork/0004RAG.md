# RAG 파이프라인 구축하기

## 1. 환경구축
### 라이브러리 임포트
```
!pip install langchain langchain-community langchain-openai chromadb tiktoken -q

import pandas as pd
import numpy as np
import os
import openai

from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage


import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
```


### OpenAI key 등록
```
def load_api_keys(filepath=".txt"):
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if line and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key.strip()] = value.strip()

path = ''

# API 키 로드 및 환경변수 설정
load_api_keys(path + '.txt')
```


---
## Vector DB 구성
### Loader
```
# text 문서 로드 임포트
from langchain.document_loaders import TextLoader
# PDF 문서 로드 임포트
from langchain.document_loaders import PyMuPDFLoader
# csv 문서 로드 임포트
from langchain.document_loaders import CSVLoader


# 텍스트 파일 경로 지정
file_path = ".txt"

# TextLoader를 이용하여 문서 로드
loader = TextLoader(path+file_path)
# PDF 문서 로드
loader = PyMuPDFLoader(path + pdf_path)
# CSV 문서 로드
loader = CSVLoader(file_path= path + csv_path)

documents = loader.load()

# 로드된 문서 출력
print(documents)
```

### Splitter
```
text_splitter = CharacterTextSplitter(
    chunk_size = 500,
    chunk_overlap  = 100,
    separator = '',   
)

split_texts = text_splitter.split_text(text)
```


### Embedding
```
# 임베딩 모델 모듈
from langchain.embeddings import OpenAIEmbeddings
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# chroma vec db 모듈
from langchain.vectorstores import Chroma
vectorstore = Chroma.from_texts(split_texts, embedding_model, persist_directory="./chroma_db")

# faiss vec db 모듈
from langchain.vectorstores import FAISS
vectorstore = FAISS.from_documents(split_docs, embedding_model)
vectorstore.save_local("faiss_index")  # 'faiss_index' 폴더에 저장됨
```


### 유사도 검색
```
query = ""
retrieved_docs = vectorstore.similarity_search(query, k=3)

# 결과 출력
print("검색 결과:")
for doc in retrieved_docs:
    print(doc.page_content)
    print('-'*200)
```
---
## RAG 파이프라인 구축
### Retriever
```
# Retriever 생성 (vec db의 것을 사용함)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
```

### llm 모델 지정
```
# 모델 설정
llm = ChatOpenAI(model_name="gpt-4.1-mini")
```

### 메모리 선언
```
# 메모리 생성
from langchain.chains import ConversationChain

# 메모리 추가 
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")
```

### 체인 함수로 엮기
```
# 모듈 추가
from langchain.chains import ConversationalRetrievalChain

# RAG 기반 ConversationalRetrievalChain 구성
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=False  # 검색된 문서 출력 옵션
)

# 테스트 실행
query = ""
response = qa_chain({"question": query})

# 응답 출력
print("답변:", response["answer"])
```