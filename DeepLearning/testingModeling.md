# 모델 성능 최적화
---
## 모델링의 목적
### 모델링의 목적
- 표본을 학습해서 전체 집단에 대해 일반화 하기 위해서

### 모델의 복잡도
- 너무 단순 : 언더피트
- 적잘함 : 일반화
- 너무 복잡 : 오버피트

**오버피트 문제**
- 학습한 내용만 맞다고 해서 일반화가 잘 일어나지 않음
    - 모델링의 목적은 지금 학습내용을 잘 맞추겠다가 아니라 앞으로 새로운 게 왔을 때 잘 맞추는, 일반화를 잘 시키기 위해서

**성능 최적화와 과적합의 관계**
- 학습 곡선을 볼 때
    1. 학습 오차 차이가 낮아지고
    2. 검증 오차 차이가 커지면
- 과적합 상태임을 말한다

### 적절한 모델 만드는 법
**적절한 복잡도 지점 찾기**
- 복잡도를 조절함
- train loss와 validation loss를 측정하고 비교

**딥러닝에서 조절 대상**
- 학습과정 : epochs, learning_rate, batch_size,
- 모델 구조 : hidden layer, node
- 규제 관련 : dropout, L1/L2 Reqularization

**딥러닝 모델 복잡도**
- 딥러닝 모델의 복잡도 혹은 규모와 관련된 수는 파라미터임
    - 파라미터 수가 많을수록 복잡한 모델이 완성됨
    - 다만 표현력은 좋지만 과적합 위험이 높아짐
- 파라미터 수 증가는 메모리 사용량과 연산 속도에도 직접적인 영향을 줌
---
## Early Stopping
반복 횟수가 많으면 과적합 될 수 있음
- 이를 예방하기 위해 Early Stopping을 사용
- 일반적으로 Train Loss는 계속 감소

### Early Stopping 요소
**Monitor=val_loss**
- 학습 과정에서 모니터링 할 값을 지정하는 옵션

**min_delta=0.01**
- 변화량 최소 기준, 이 이상 좋아지지 않으면 개선이 없다고 판단

**patience=10**
- 얼마나 참을지 설정하는 옵션

**callbacks=[es]**
- .fit() 안에 지정

---
## Dropout
- 과적합을 줄이기 위한 정규화 기법 중 하나
- 학습 중 일정비율의 뉴런을 무작위로 비활성화시켜, 모델의 일반화 성능을 향상시킴

**학습 시 적용 절차**
- 매 훈련 배치마다 일부 뉴런을 무작위로 선택해 비활성화
- 비활성화된 뉴런은 해당 배치에서 순전파와 역전파에 참여하지 않음
- 이를 통해 뉴런간의 과도한 의존성을 줄이고, 보다 강건한 모델을 학습